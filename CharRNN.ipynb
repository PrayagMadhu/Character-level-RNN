{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "vocab=sorted(set(text))\n",
    "vocab2int={j:i for i, j in enumerate(vocab)}\n",
    "int2vocab=dict(enumerate(vocab))\n",
    "encoded=np.array([vocab2int[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    \n",
    "    char_per_batch=batch_size*n_steps\n",
    "    n_batches=len(arr)//char_per_batch\n",
    "    arr=arr[:n_batches*char_per_batch] #no. of elements to keep in array to get full batches\n",
    "    arr=arr.reshape((batch_size, -1)) # reshaping linear array to N X M type\n",
    "    \n",
    "    for i in range(0, arr.shape[1], n_steps):\n",
    "        x=arr[:, i:i+n_steps]\n",
    "        y_tmp=arr[:, i+1:i+n_steps+1]\n",
    "        y=np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:, :y_tmp.shape[1]]=y_tmp\n",
    "        yield x, y\n",
    "\n",
    "batches=get_batches(encoded, 12, 3)\n",
    "x,y=next(batches)\n",
    "z,p=next(batches)\n",
    "type(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, n_steps):\n",
    "    inputs=tf.placeholder(tf.int32, [batch_size, n_steps])\n",
    "    targets=tf.placeholder(tf.int32, [batch_size, n_steps])\n",
    "    keep_prob=tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob\n",
    "\n",
    "def build_lstm(lstm_size, n_layers, batch_size, keep_prob):\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm_cells=tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        dropOut=tf.contrib.rnn.DropoutWrapper(lstm_cells, output_keep_prob=keep_prob)\n",
    "        \n",
    "        return dropOut\n",
    "    \n",
    "    #stacks up multiple LSTM cells\n",
    "    cells=tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(n_layers)])\n",
    "    initial_state=cells.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cells, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_outputs(lstm_output, in_size, out_size):\n",
    "    #in_size=size of lstm cells\n",
    "    #out_size=size of o/p softmax layer\n",
    "    \n",
    "    seq_out=tf.concat(lstm_output, axis=1)#concatenate NXMXL (3D) to (MXN)XL(2D)\n",
    "    x=tf.reshape(seq_out, [-1, in_size])#reshapes seq_out such that 4 each (MXN rows) a column of lstm output\n",
    "                                         #(in_size) occurs\n",
    "    \n",
    "    #connect RNN output to softmax layers\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w=tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b=tf.Variable(tf.zeros(out_size))\n",
    "        \n",
    "    logits=tf.add(tf.matmul(x, softmax_w), softmax_b)\n",
    "    out= tf.nn.softmax(logits, name='predicctions')\n",
    "    \n",
    "    return logits, out\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, n_classes):\n",
    "    y_onehot=tf.one_hot(targets, n_classes)\n",
    "    y_reshaped=tf.reshape(y_onehot, logits.get_shape())\n",
    "    \n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_optm(loss, learn_rate, grad_clip):\n",
    "    tvars=tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learn_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    return optimizer\n",
    "\n",
    "def pickTop_n(pred, vocab_size, top_n=5):\n",
    "    p=np.squeeze(pred)\n",
    "    p[np.argsort(p)[:-top_n]]=0\n",
    "    p=p/np.sum(p)\n",
    "    char=np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_step=50, lstm_size=128,\n",
    "                num_layers=2, learning_rate=1e-3, grad_clip=5, sampling=False):\n",
    "        \n",
    "        if sampling==True:\n",
    "            num_step=1\n",
    "            batch_size=1\n",
    "        else:\n",
    "            num_step=num_step\n",
    "            batch_size=batch_size\n",
    "            \n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs, self.targets, self.keep_prob=build_inputs(batch_size, num_step)\n",
    "        cell, self.initial_state=build_lstm(lstm_size,num_layers,batch_size, keep_prob)\n",
    "        \n",
    "        x_oneHot=tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        outputs, state= tf.nn.dynamic_rnn(cell, x_oneHot, initial_state=self.initial_state)\n",
    "        self.final_state=state\n",
    "        \n",
    "        self.logits, self.prediction=build_outputs(outputs, lstm_size, num_classes)\n",
    "        self.loss=build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optmizer=build_optm(self.loss, learning_rate, grad_clip)\n",
    "        \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dro_enter__pout keep probability\n",
    "num_classes=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-d77a001d5770>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1757...  0.3157 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.1016...  0.3139 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.7698...  0.3164 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4531...  0.3134 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3361...  0.3130 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2126...  0.3119 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.1740...  0.3162 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0461...  0.3163 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 1.9830...  0.3169 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9207...  0.3163 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.8883...  0.3183 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.7841...  0.3143 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8048...  0.3131 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7435...  0.3123 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7159...  0.3159 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.6703...  0.3175 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6345...  0.3321 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6299...  0.3125 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6078...  0.3301 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.5750...  0.3205 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.5996...  0.3141 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.5521...  0.3353 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.5424...  0.3853 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.4863...  0.3132 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5433...  0.3894 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.4481...  0.3850 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.4528...  0.3155 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.4734...  0.3136 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4252...  0.3167 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.3931...  0.3139 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.3949...  0.3134 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.3579...  0.3131 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.3807...  0.3149 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3279...  0.3134 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.3453...  0.3271 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.3745...  0.3394 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3161...  0.3196 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.3331...  0.3314 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.3875...  0.3225 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.3475...  0.3165 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3052...  0.3293 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.2921...  0.3188 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3017...  0.3133 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3101...  0.3187 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3106...  0.3361 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.2523...  0.3262 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.2448...  0.3261 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.2754...  0.3393 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.2431...  0.3116 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.2560...  0.3188 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.2546...  0.3238 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2135...  0.3504 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.2658...  0.3344 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.1942...  0.3176 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2110...  0.3593 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.2369...  0.3437 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2220...  0.3210 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2307...  0.3167 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.2570...  0.3146 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.2307...  0.3183 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2185...  0.3209 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.1686...  0.3130 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.1752...  0.3178 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.1762...  0.3120 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.1986...  0.3134 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.1826...  0.3160 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.1952...  0.3122 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2030...  0.3153 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.1893...  0.3131 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.1958...  0.3207 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.1791...  0.3207 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.1806...  0.3390 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.1774...  0.3152 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.1773...  0.3406 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.1578...  0.3302 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1405...  0.3162 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.1564...  0.3122 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.1792...  0.3142 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.1680...  0.3154 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "print_every=50\n",
    "save_every=200\n",
    "model=CharRNN(num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate)\n",
    "saver=tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter=0\n",
    "    for epoch in range(epochs):\n",
    "        new_state=sess.run(model.initial_state)\n",
    "        batch_loss=0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter+=1\n",
    "            start=time.time()\n",
    "            feed={model.inputs:x, \n",
    "                  model.targets:y, \n",
    "                  model.keep_prob:keep_prob,\n",
    "                  model.initial_state:new_state}\n",
    "            \n",
    "            batch_loss, new_state, _= sess.run([model.loss, \n",
    "                                                model.final_state, \n",
    "                                                model.optmizer], feed_dict=feed)\n",
    "            \n",
    "            \n",
    "            if (counter%print_every==0):\n",
    "                end=time.time()\n",
    "                print('Epoch: {}/{}... '.format(epoch+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "\n",
    "            if(counter%save_every==0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples(checkpoints, n_samples, lstm_size, vocab_size, prime='the '):\n",
    "    samples=[i for i in prime]\n",
    "    model=CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "        new_state=sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x=np.zeros((1,1))\n",
    "            x[0]=vocab2int[c]\n",
    "            feed={model.inputs:x,\n",
    "                 model.keep_prob:1.0,\n",
    "                 model.initial_state:new_state}\n",
    "            pred, new_state=sess.run([model.prediction,model.final_state], feed_dict=feed)\n",
    "            \n",
    "        c=pickTop_n(pred, len(vocab))\n",
    "        samples.append(int2vocab[c])\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            x[0,0]=c\n",
    "            feed={model.inputs:x,\n",
    "                 model.keep_prob:1.0,\n",
    "                 model.initial_state:new_state}\n",
    "            pred, new_state=sess.run([model.prediction,model.final_state], feed_dict=feed)\n",
    "            c=pickTop_n(pred, len(vocab))\n",
    "            samples.append(int2vocab[c])\n",
    "    \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-dbfd0736dab3>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i3960_l512.ckpt\n",
      "may told you in a meaning show, that sudeend hange to meet the\n",
      "trouble with all his which he stepped on the posision of the peasantry.\n",
      "He had thenesely to beach the superations of childhe had been already\n",
      "satts.\n",
      "\n",
      "\"When? You could not live human in the form in the counhonach the\n",
      "subjects\n",
      "of the project will, what is it you was looking always be there.\n",
      "Wheleres all the district asparronsed the mensaigs, is in\n",
      "a complation and morning. He sent that, to do it wan a sort\n",
      "with which they had been the face in the children, and she was already,\n",
      "she saw that this would secret her an absamety for shale to that in the\n",
      "sight. She hed out that her long coulded, try off any, though the\n",
      "peanant fingers were, she higher he had been an evening anything,\n",
      "but the same time. Sergey Ivanovitch saw with home.\n",
      "\n",
      "The pretime the som that he could not the thing. I speak of minding\n",
      "them to the thrust of the peasantry that it was as that to get the\n",
      "same too, and she had not love for havingssalest treed to be a fact and\n",
      "starling, which irmitiaded an una still marshel of clever always,\n",
      "soligicelently, and hat had booked by the cause of the capiaation.\n",
      "\n",
      "\"Oh, you won't the crivilounger for a course, but you've been told\n",
      "me.\"\n",
      "\n",
      "\"You've been triem, I see and think I'm shon in the person would she to\n",
      "be me to say... I shall be diverted out in a report. It's all their\n",
      "having bored of him,\" said Stepan Arkadyevitch. \"I don't know wheel.\n",
      "I'll tell you here; they're told you to meet you.\"\n",
      "\n",
      "\"No, thankov one?.. Alexey.\"\n",
      "\n",
      "All the sugerasions of the whisker with the profry of this laster was\n",
      "the lady. She wan to see herself in the station worders, was sore in\n",
      "his heart she wasnets those from the step would have althey and study\n",
      "of the light state. But thought attoul he too; he cried her high\n",
      "starsh across.\n",
      "\n",
      "Still, and alone, and an instant he had concealed to her order. This\n",
      "counarly happy had cleared alone the stress of the chood and again a\n",
      "position. The party and her properdy called an idoa of light and was\n",
      "th\n"
     ]
    }
   ],
   "source": [
    "checkpoints=tf.train.latest_checkpoint('checkpoints')\n",
    "samples=generateSamples(checkpoints, 2000, lstm_size, len(vocab), prime='may')\n",
    "print(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
